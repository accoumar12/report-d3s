@article{gaoDeepNeuralNetwork2022,
  title = {Deep {{Neural Network}} for {{3D Shape Classification Based}} on {{Mesh Feature}}},
  author = {Gao, Mengran and Ruan, Ningjun and Shi, Junpeng and Zhou, Wanli},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {18},
  pages = {7040},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22187040},
  urldate = {2024-05-24},
  abstract = {Virtual reality, driverless cars, and robotics all make extensive use of 3D shape classification. One of the most popular ways to represent 3D data is with polygonal meshes. In particular, triangular mesh is frequently employed. A triangular mesh has more features than 3D data formats such as voxels, multi-views, and point clouds. The current challenge is to fully utilize and extract useful information from mesh data. In this paper, a 3D shape classification network based on triangular mesh and graph convolutional neural networks was suggested. The triangular face of this model was viewed as a unit. By obtaining an adjacency matrix from mesh data, graph convolutional neural networks can be utilized to process mesh data. The studies were performed on the ModelNet40 dataset with an accuracy of 91.0\%, demonstrating that the classification network in this research may produce effective results.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {3D shape classification,graph convolutional neural networks,P1,triangular mesh},
  file = {C:\Users\maccou\Zotero\storage\4M4JGZIU\Gao et al. - 2022 - Deep Neural Network for 3D Shape Classification Ba.pdf}
}

@incollection{geDeepMetricLearning2018,
  title = {Deep {{Metric Learning}} with {{Hierarchical Triplet Loss}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2018},
  author = {Ge, Weifeng and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11210},
  pages = {272--288},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01231-1_17},
  urldate = {2024-05-23},
  abstract = {We present a novel hierarchical triplet loss (HTL) capable of automatically collecting informative training samples (triplets) via a defined hierarchical tree that encodes global context information. This allows us to cope with the main limitation of random sampling in training a conventional triplet loss, which is a central issue for deep metric learning. Our main contributions are two-fold. (i) we construct a hierarchical class-level tree where neighboring classes are merged recursively. The hierarchical structure naturally captures the intrinsic data distribution over the whole dataset. (ii) we formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically based on the designed hierarchical tree. This allows it to automatically select meaningful hard samples with the guide of global context. It encourages the model to learn more discriminative features from visual similar classes, leading to faster convergence and better performance. Our method is evaluated on the tasks of image retrieval and face recognition, where it can obtain comparable performance with much fewer iterations. It outperforms the standard triplet loss substantially by 1\% - 18\%, and achieves new state-of-the-art performance on a number of benchmarks.},
  isbn = {978-3-030-01230-4 978-3-030-01231-1},
  langid = {english},
  file = {C:\Users\maccou\Zotero\storage\XPUSCL4Q\Ge et al. - 2018 - Deep Metric Learning with Hierarchical Triplet Los.pdf}
}

@misc{hermansDefenseTripletLoss2017,
  title = {In {{Defense}} of the {{Triplet Loss}} for {{Person Re-Identification}}},
  author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  year = {2017},
  month = nov,
  number = {arXiv:1703.07737},
  eprint = {1703.07737},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.07737},
  urldate = {2024-05-28},
  abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\5J9W9DU6\\Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf;C\:\\Users\\maccou\\Zotero\\storage\\7CGR3X7L\\1703.html}
}

@misc{liuOpenShapeScaling3D2023,
  title = {{{OpenShape}}: {{Scaling Up 3D Shape Representation Towards Open-World Understanding}}},
  shorttitle = {{{OpenShape}}},
  author = {Liu, Minghua and Shi, Ruoxi and Kuang, Kaiming and Zhu, Yinhao and Li, Xuanlin and Han, Shizhong and Cai, Hong and Porikli, Fatih and Su, Hao},
  year = {2023},
  month = jun,
  number = {arXiv:2305.10764},
  eprint = {2305.10764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.10764},
  urldate = {2024-06-11},
  abstract = {We introduce OpenShape, a method for learning multi-modal joint representations of text, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisy text descriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negative mining module for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of 46.8\% on the 1,156-category Objaverse-LVIS benchmark, compared to less than 10\% for existing methods. OpenShape also achieves an accuracy of 85.3\% on ModelNet40, outperforming previous zero-shot baseline methods by 20\% and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIP-based models for various applications, such as point cloud captioning and point cloud-conditioned image generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\WZ5YJ9TB\\Liu et al. - 2023 - OpenShape Scaling Up 3D Shape Representation Towa.pdf;C\:\\Users\\maccou\\Zotero\\storage\\9YLGUHFY\\2305.html}
}

@article{popRotationInvariantGraph2023,
  title = {Rotation {{Invariant Graph Neural Network}} for {{3D Point Clouds}}},
  author = {Pop, Alexandru and Dom{\textcommabelow s}a, Victor and Tamas, Levente},
  year = {2023},
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {5},
  pages = {1437},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs15051437},
  urldate = {2024-04-29},
  abstract = {In this paper we propose a novel rotation normalization technique for point cloud processing using an oriented bounding box. We use this method to create a point cloud annotation tool for part segmentation on real camera data. Custom data sets are used to train our network for classification and part segmentation tasks. Successful deployment is completed on an embedded device with limited processing power. A comparison is made with other rotation-invariant features in noisy synthetic datasets. Our method offers more auxiliary information related to the dimension, position, and orientation of the object than previous methods while performing at a similar level.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {classification,computer vision,object part segmentation},
  file = {C:\Users\maccou\Zotero\storage\CXCVTJ6L\Pop et al. - 2023 - Rotation Invariant Graph Neural Network for 3D Poi.pdf}
}

@misc{qianPointNeXtRevisitingPointNet2022,
  title = {{{PointNeXt}}: {{Revisiting PointNet}}++ with {{Improved Training}} and {{Scaling Strategies}}},
  shorttitle = {{{PointNeXt}}},
  author = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan Abed Al Kader and Elhoseiny, Mohamed and Ghanem, Bernard},
  year = {2022},
  month = oct,
  number = {arXiv:2206.04670},
  eprint = {2206.04670},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.04670},
  urldate = {2024-06-07},
  abstract = {PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9\% to 86.1\%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7 on ScanObjectNN, surpassing PointMLP by 2.3\%, while being 10x faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9\% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\WEIFKS8X\\Qian et al. - 2022 - PointNeXt Revisiting PointNet++ with Improved Tra.pdf;C\:\\Users\\maccou\\Zotero\\storage\\E3HY5Y8D\\2206.html}
}

@misc{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  shorttitle = {{{PointNet}}++},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = jun,
  number = {arXiv:1706.02413},
  eprint = {1706.02413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.02413},
  urldate = {2024-06-07},
  abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\6B5GN7PL\\Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on .pdf;C\:\\Users\\maccou\\Zotero\\storage\\7LWXYEUH\\1706.html}
}

@misc{qiPointNetDeepLearning2017,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  year = {2017},
  month = apr,
  number = {arXiv:1612.00593},
  eprint = {1612.00593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.00593},
  urldate = {2024-06-07},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\8XWPFWUU\\Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf;C\:\\Users\\maccou\\Zotero\\storage\\ANX9UMRX\\1612.html}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2024-09-16},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\BXGI7KNN\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\maccou\\Zotero\\storage\\HGRKKQJD\\2103.html}
}

@misc{robinsonContrastiveLearningHard2021,
  title = {Contrastive {{Learning}} with {{Hard Negative Samples}}},
  author = {Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
  year = {2021},
  month = jan,
  number = {arXiv:2010.04592},
  eprint = {2010.04592},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-23},
  abstract = {How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,P1,Statistics - Machine Learning},
  file = {C:\Users\maccou\Zotero\storage\53M2N93D\Robinson et al. - 2021 - Contrastive Learning with Hard Negative Samples.pdf}
}

@misc{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  eprint = {1503.03832},
  primaryclass = {cs},
  doi = {10.1109/CVPR.2015.7298682},
  urldate = {2024-05-28},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\9SAEEEJE\\Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;C\:\\Users\\maccou\\Zotero\\storage\\ZHT8MKJW\\1503.html}
}

@misc{shiPointGNNGraphNeural2020,
  title = {Point-{{GNN}}: {{Graph Neural Network}} for {{3D Object Detection}} in a {{Point Cloud}}},
  shorttitle = {Point-{{GNN}}},
  author = {Shi, Weijing and Ragunathan and Rajkumar},
  year = {2020},
  month = mar,
  number = {arXiv:2003.01251},
  eprint = {2003.01251},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-07},
  abstract = {In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud. Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph. We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately. Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms. Our results demonstrate the potential of using the graph neural network as a new approach for 3D object detection. The code is available at https://github.com/WeijingShi/Point-GNN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\maccou\Zotero\storage\JDB77SKH\Shi et al. - 2020 - Point-GNN Graph Neural Network for 3D Object Dete.pdf}
}

@article{sundriyalSemiSupervisedLearningTriplet2021,
  title = {Semi-{{Supervised Learning}} via {{Triplet Network Based Active Learning}} ({{Student Abstract}})},
  author = {Sundriyal, Divyanshu and Ghosh, Soumyadeep and Vatsa, Mayank and Singh, Richa},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {18},
  pages = {15903--15904},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i18.17948},
  urldate = {2024-05-14},
  abstract = {In recent years, deep learning models have pushed state-ofthe-art accuracies for several machine learning tasks. However, such models require a large amount of (supervised) data for training. While unlabelled data is available in abundance, manually labeling them is very costly. Active learning techniques helps in utilizing unlabelled data which may result in an improved classification model. In this research, we present an active learning algorithm which can help in increasing performance of deep learning models by using large amount of unlabelled data. A novel active learning algorithm, Triplet AL is proposed which uses a triplet network to select samples from an unlabelled data set. Previous active learning methods rely on classification model's final prediction scores as a measure of confidence for an unlabelled sample. We propose a more reliable confidence measure, termed as Top-Two-Margin which is given by the Triplet Network. The proposed algorithm outperforms other active learning approaches which are used to compare in this research.},
  langid = {english},
  file = {C:\Users\maccou\Zotero\storage\PE9XANNY\Sundriyal et al. - 2021 - Semi-Supervised Learning via Triplet Network Based.pdf}
}

@misc{vermaFeaStNetFeatureSteeredGraph2018,
  title = {{{FeaStNet}}: {{Feature-Steered Graph Convolutions}} for {{3D Shape Analysis}}},
  shorttitle = {{{FeaStNet}}},
  author = {Verma, Nitika and Boyer, Edmond and Verbeek, Jakob},
  year = {2018},
  month = mar,
  number = {arXiv:1706.05206},
  eprint = {1706.05206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.05206},
  urldate = {2024-06-10},
  abstract = {Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\RWV6IVHP\\Verma et al. - 2018 - FeaStNet Feature-Steered Graph Convolutions for 3.pdf;C\:\\Users\\maccou\\Zotero\\storage\\UC97I8MP\\1706.html}
}

@misc{wangDynamicGraphCNN2019,
  title = {Dynamic {{Graph CNN}} for {{Learning}} on {{Point Clouds}}},
  author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  year = {2019},
  month = jun,
  number = {arXiv:1801.07829},
  eprint = {1801.07829},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.07829},
  urldate = {2024-06-07},
  abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\KF9CVZFF\\Wang et al. - 2019 - Dynamic Graph CNN for Learning on Point Clouds.pdf;C\:\\Users\\maccou\\Zotero\\storage\\BTGSX4E7\\1801.html}
}

@misc{wengContrastiveRepresentationLearning2021,
  title = {Contrastive {{Representation Learning}}},
  author = {Weng, Lilian},
  year = {2021},
  month = may,
  urldate = {2024-09-16},
  abstract = {The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning. Contrastive Training Objectives In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2021-05-31-contrastive/},
  langid = {english},
  file = {C:\Users\maccou\Zotero\storage\R7K8W8N5\2021-05-31-contrastive.html}
}

@misc{wengSelfSupervisedRepresentationLearning2019,
  title = {Self-{{Supervised Representation Learning}}},
  author = {Weng, Lilian},
  year = {2019},
  month = nov,
  urldate = {2024-05-23},
  abstract = {[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding]. [Updated on 2020-04-13: add a ``Momentum Contrast'' section on MoCo, SimCLR and CURL.] [Updated on 2020-07-08: add a ``Bisimulation'' section on DeepMDP and DBC.] [Updated on 2020-09-12: add MoCo V2 and BYOL in the ``Momentum Contrast'' section.] [Updated on 2021-05-31: remove section on ``Momentum Contrast'' and add a pointer to a full post on ``Contrastive Representation Learning''] Given a task and enough labels, supervised learning can solve it really well.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2019-11-10-self-supervised/},
  langid = {english},
  file = {C:\Users\maccou\Zotero\storage\E3ZSD7MT\2019-11-10-self-supervised.html}
}

@misc{yangFoldingNetPointCloud2018,
  title = {{{FoldingNet}}: {{Point Cloud Auto-encoder}} via {{Deep Grid Deformation}}},
  shorttitle = {{{FoldingNet}}},
  author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
  year = {2018},
  month = apr,
  number = {arXiv:1712.07262},
  eprint = {1712.07262},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.07262},
  urldate = {2024-06-26},
  abstract = {Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7\% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license\#FoldingNet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\6CL33JBT\\Yang et al. - 2018 - FoldingNet Point Cloud Auto-encoder via Deep Grid.pdf;C\:\\Users\\maccou\\Zotero\\storage\\BC4DTTYA\\1712.html}
}

@misc{yuPointBERTPretraining3D2022,
  title = {Point-{{BERT}}: {{Pre-training 3D Point Cloud Transformers}} with {{Masked Point Modeling}}},
  shorttitle = {Point-{{BERT}}},
  author = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
  year = {2022},
  month = jun,
  number = {arXiv:2111.14819},
  eprint = {2111.14819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.14819},
  urldate = {2024-06-11},
  abstract = {We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8\% accuracy on ModelNet40 and 83.1\% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\ZC7ALGHT\\Yu et al. - 2022 - Point-BERT Pre-training 3D Point Cloud Transforme.pdf;C\:\\Users\\maccou\\Zotero\\storage\\L76ZYK9K\\2111.html}
}

@article{zhangDeepLearningbased3D2023,
  title = {Deep {{Learning-based 3D Point Cloud Classification}}: {{A Systematic Survey}} and {{Outlook}}},
  shorttitle = {Deep {{Learning-based 3D Point Cloud Classification}}},
  author = {Zhang, Huang and Wang, Changshuo and Tian, Shengwei and Lu, Baoli and Zhang, Liping and Ning, Xin and Bai, Xiao},
  year = {2023},
  month = sep,
  journal = {Displays},
  volume = {79},
  eprint = {2311.02608},
  primaryclass = {cs},
  pages = {102456},
  issn = {01419382},
  doi = {10.1016/j.displa.2023.102456},
  urldate = {2024-05-13},
  abstract = {In recent years, point cloud representation has become one of the research hotspots in the field of computer vision, and has been widely used in many fields, such as autonomous driving, virtual reality, robotics, etc. Although deep learning techniques have achieved great success in processing regular structured 2D grid image data, there are still great challenges in processing irregular, unstructured point cloud data. Point cloud classification is the basis of point cloud analysis, and many deep learning-based methods have been widely used in this task. Therefore, the purpose of this paper is to provide researchers in this field with the latest research progress and future trends. First, we introduce point cloud acquisition, characteristics, and challenges. Second, we review 3D data representations, storage formats, and commonly used datasets for point cloud classification. We then summarize deep learning-based methods for point cloud classification and complement recent research work. Next, we compare and analyze the performance of the main methods. Finally, we discuss some challenges and future directions for point cloud classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\6BGGYLM4\\Zhang et al. - 2023 - Deep Learning-based 3D Point Cloud Classification.pdf;C\:\\Users\\maccou\\Zotero\\storage\\F3YBY5AU\\2311.html}
}

@misc{zhaoPointTransformer2021,
  title = {Point {{Transformer}}},
  author = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip and Koltun, Vladlen},
  year = {2021},
  month = sep,
  number = {arXiv:2012.09164},
  eprint = {2012.09164},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.09164},
  urldate = {2024-06-07},
  abstract = {Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4\% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70\% mIoU threshold for the first time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\maccou\\Zotero\\storage\\X9MDDK6A\\Zhao et al. - 2021 - Point Transformer.pdf;C\:\\Users\\maccou\\Zotero\\storage\\YZLY8TKK\\2012.html}
}
